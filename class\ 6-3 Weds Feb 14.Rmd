---
title: "Class 6-3"
output: html_document
---

```{r setup, include=FALSE}
# install.packages("tidyverse")
# install.packages("mosaic")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("broom")
# install.packages("lattice")
# install.packages("gridExtra")
# install.packages("grid")
# install.packages("devtools")
# devtools::install_github("ProjectMOSAIC/mosaicModel")
# install.packages("pryr")

#library(pryr) # used for debugging function evaluation
library(tidyverse)
library(mosaic)
library(ggplot2)
library(dplyr)
library(broom)
library(lattice)
library(gridExtra)
library(grid)
library(mosaicModel) # newer version of statisticalModeling with better functions



```

#Load ads data

```{r}
ads_sales <- as_tibble(read.csv("Advertising.csv"))

```

## Resample function

```{r}
?resample
fruit <- c("apple", "kiwi", "lemon", "orange", "pomegranate")
rep(1/length(fruit), times=length(fruit))
```

## Let's take some resamples from the fruit

You can also embed plots, for example:

```{r pressure, echo=FALSE}
fruit
resample(fruit)
resample(fruit)
resample(fruit)
resample(fruit)

```
We see that we don't always get the same sample. This is because resample is taking any row with equal probability, then doing it again. It can choose a row many times. If this makes you nervous, fear not, as the dataset size increases, the chance that you choose a row over and over (too much) is quite small.

Typically you should use 15,000 resamples to make a sampling distribution. With modern computers this is often quite feasible. (For big data, you can take a sample of your data 15,000 times.)

# "Bootstrapping" using 3 resamples as an example

```{r}
# let's use the ads dataset from last class
# let's make a confidence interval for the effect size of TV

approx_sampling_distribution_for_TV_effect <- tribble( ~slope)

resample1 <- resample(ads_sales)
model <- lm ( sales ~ TV + radio + newspaper, data=resample1)
slope <- mod_effect(model, ~ TV) %>% select(slope)
approx_sampling_distribution_for_TV_effect <- 
  bind_rows(approx_sampling_distribution_for_TV_effect,slope)


# how do we interpret effect size with multiple explanatory variables / interaction terms in the model
slope
approx_sampling_distribution_for_TV_effect

resample2 <- resample(ads_sales)
model <- lm ( sales ~ TV + radio + newspaper, data=resample2)
slope <- mod_effect(model, ~ TV) %>% select(slope)
approx_sampling_distribution_for_TV_effect <- 
  bind_rows(approx_sampling_distribution_for_TV_effect,slope)

slope
approx_sampling_distribution_for_TV_effect

# 14,998 more times ...
```
# mod_ensemble - makes <nreps> of models for you

mod_ensemble uses bootstrapped resample of the data for each model
```{r}
model <- lm ( sales ~ TV + radio + newspaper, data=ads_sales)

#using resample1 and resample2 above, that's what mod_ensemble does
# here's roughly equivalent code
sales_mod_ensemble <- mod_ensemble(model, nreps = 2) 
approx_sampling_distribution_w_mod_ensemble <- 
  mod_effect(sales_mod_ensemble, ~ TV)

# now let's do an acutal full bootstrapping example
# we're going to try to get sampling distribution for effect size of TV

sales_mod_ensemble <- mod_ensemble(model, nreps = 1000) #you should use 15k in practice
approx_sampling_distribution <- 
  mod_effect(sales_mod_ensemble, ~ TV)

approx_sampling_distribution
hist(approx_sampling_distribution$slope)


# you can use empirical quantiles
# ie just find the range of the data falls within
# a 100%-alpha confidence interval (here alpha = .05)
# you do 

quantile(approx_sampling_distribution$slope, probs = c(.025, .975))

# here's a function to get confidence interval, as a function of alpha
confidence_interval_for <- function(actual_column, alpha){
  lower <- alpha/2
  upper <- 1 - (alpha/2)
  quantile(actual_column, probs = c(lower, upper))
}

# this gives same as 
# quantile(approx_sampling_distribution$slope, probs = c(.025, .975))
confidence_interval_for(approx_sampling_distribution$slope, .05)

# you can also use parametric formulas
lower_ci =  mean (approx_sampling_distribution$slope) - 1.96*sd(approx_sampling_distribution$slope)
upper_ci =  mean (approx_sampling_distribution$slope) + 1.96*sd(approx_sampling_distribution$slope)


#compare to the theoretical CI (from confint)
confint(model, "TV")
confidence_interval_for(approx_sampling_distribution$slope, .05)
c(lower_ci, upper_ci )

ci <- confidence_interval_for(approx_sampling_distribution$slope, .05)

ggplot(approx_sampling_distribution, aes(slope)) +
  geom_histogram() + 
  geom_vline(xintercept = ci[1], color="red" ) + # lower CI value
  geom_vline(xintercept = ci[2], color="red" ) # upper CI value
  
# the linear model's assumptions don't hold for this model
# b/c if you look at the residuals, you see they don't look
# gaussian distributed.

data_with_residuals <- augment(model, ads_sales)
#go ahead and add a plot for the residuals here

# You can also test the residuals for normality
# with this test
shapiro.test(data_with_residuals$.resid)

#how to tell what the p-value means for the shapiro wilkes test above
# does p<alpha (eg .05) mean the data is not normal?
#let's do a monte carlo simulation
# generate some normal data test it, then compare to non-normal data
shapiro.test(rnorm(100, mean = 5, sd = 3))
shapiro.test(runif(100, min = 2, max = 4))

# the p-value is much lower for the second one
# (the not gaussian data one), so we can tell that p<alpha means
# rejecting the null hypothesis that the data is normal

# so, are the residuals normal for our linear model?
# if not, using the math-derived confint function is not appropriate
# / we would expect math-derived confint to not estimate the population parameter
shapiro.test(data_with_residuals$.resid)

```

# Monte carlo simulation example

Simulating the time I will get to school. See slides 6.3 (Weds Week 6) for more on monte carlo.

```{r}
commute_monte_carlo <- function(){
  wakeup <- 700 + rnorm(1, 10, 2)
  breakfast <- 10 + runif(1, -5, 5 )
  commute <- 20 + runif(1, 0, 10 )
  time_to_get_to_school <- wakeup + breakfast + commute
  
  #time_to_get_to_school <-  ...
  return(time_to_get_to_school)
}

simulate_1000_times <- do(1000)*commute_monte_carlo()
hist(simulate_1000_times$commute_monte_carlo)

```
# Monte carlo simulation: Profit from buying ads when ad price might vary

```{r}
profit_monte_carlo <- function(){
  # the price of ads fluctuates with the market
  # we model these price fluctuations with a random value
  ad_cost <- 9 + runif(1, -2, 10)
  number_of_ads <- 2
  expected_sales_per_ad <- 1.04*number_of_ads
  
  profit <- expected_sales_per_ad - (ad_cost*number_of_ads)
  return(profit)
}

simulate_1000_times <- do(1000)*profit_monte_carlo()
hist(simulate_1000_times$profit_monte_carlo)
```



